{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43d1384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\langchain2\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import feedparser\n",
    "from urllib.parse import quote\n",
    "from pypdf import PdfReader\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "from neo4j import GraphDatabase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc9f9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'http://arxiv.org/abs/2510.25772v1',\n",
       "  'title': 'VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context\\n  Learning',\n",
       "  'authors': ['Baolu Li',\n",
       "   'Yiming Zhang',\n",
       "   'Qinghe Wang',\n",
       "   'Liqian Ma',\n",
       "   'Xiaoyu Shi',\n",
       "   'Xintao Wang',\n",
       "   'Pengfei Wan',\n",
       "   'Zhenfei Yin',\n",
       "   'Yunzhi Zhuge',\n",
       "   'Huchuan Lu',\n",
       "   'Xu Jia'],\n",
       "  'summary': 'Visual effects (VFX) are crucial to the expressive power of digital media,\\nyet their creation remains a major challenge for generative AI. Prevailing\\nmethods often rely on the one-LoRA-per-effect paradigm, which is\\nresource-intensive and fundamentally incapable of generalizing to unseen\\neffects, thus limiting scalability and creation. To address this challenge, we\\nintroduce VFXMaster, the first unified, reference-based framework for VFX video\\ngeneration. It recasts effect generation as an in-context learning task,\\nenabling it to reproduce diverse dynamic effects from a reference video onto\\ntarget content. In addition, it demonstrates remarkable generalization to\\nunseen effect categories. Specifically, we design an in-context conditioning\\nstrategy that prompts the model with a reference example. An in-context\\nattention mask is designed to precisely decouple and inject the essential\\neffect attributes, allowing a single unified model to master the effect\\nimitation without information leakage. In addition, we propose an efficient\\none-shot effect adaptation mechanism to boost generalization capability on\\ntough unseen effects from a single user-provided video rapidly. Extensive\\nexperiments demonstrate that our method effectively imitates various categories\\nof effect information and exhibits outstanding generalization to out-of-domain\\neffects. To foster future research, we will release our code, models, and a\\ncomprehensive dataset to the community.',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2510.25772v1.pdf',\n",
       "  'published': '2025-10-29T17:59:53'},\n",
       " {'id': 'http://arxiv.org/abs/2510.25771v1',\n",
       "  'title': 'Gaperon: A Peppered English-French Generative Language Model Suite',\n",
       "  'authors': ['Nathan Godey',\n",
       "   'Wissam Antoun',\n",
       "   'Rian Touchent',\n",
       "   'Rachel Bawden',\n",
       "   'Éric de la Clergerie',\n",
       "   'Benoît Sagot',\n",
       "   'Djamé Seddah'],\n",
       "  'summary': 'We release Gaperon, a fully open suite of French-English-coding language\\nmodels designed to advance transparency and reproducibility in large-scale\\nmodel training. The Gaperon family includes 1.5B, 8B, and 24B parameter models\\ntrained on 2-4 trillion tokens, released with all elements of the training\\npipeline: French and English datasets filtered with a neural quality\\nclassifier, an efficient data curation and training framework, and hundreds of\\nintermediate checkpoints. Through this work, we study how data filtering and\\ncontamination interact to shape both benchmark and generative performance. We\\nfind that filtering for linguistic quality enhances text fluency and coherence\\nbut yields subpar benchmark results, and that late deliberate contamination --\\ncontinuing training on data mixes that include test sets -- recovers\\ncompetitive scores while only reasonably harming generation quality. We discuss\\nhow usual neural filtering can unintentionally amplify benchmark leakage. To\\nsupport further research, we also introduce harmless data poisoning during\\npretraining, providing a realistic testbed for safety studies. By openly\\nreleasing all models, datasets, code, and checkpoints, Gaperon establishes a\\nreproducible foundation for exploring the trade-offs between data curation,\\nevaluation, safety, and openness in multilingual language model development.',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2510.25771v1.pdf',\n",
       "  'published': '2025-10-29T17:59:39'},\n",
       " {'id': 'http://arxiv.org/abs/2510.25770v1',\n",
       "  'title': 'E-Scores for (In)Correctness Assessment of Generative Model Outputs',\n",
       "  'authors': ['Guneet S. Dhillon',\n",
       "   'Javier González',\n",
       "   'Teodora Pandeva',\n",
       "   'Alicia Curth'],\n",
       "  'summary': \"While generative models, especially large language models (LLMs), are\\nubiquitous in today's world, principled mechanisms to assess their\\n(in)correctness are limited. Using the conformal prediction framework, previous\\nworks construct sets of LLM responses where the probability of including an\\nincorrect response, or error, is capped at a desired user-defined tolerance\\nlevel. However, since these methods are based on p-values, they are susceptible\\nto p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the\\nguarantees. We therefore leverage e-values to complement generative model\\noutputs with e-scores as a measure of incorrectness. In addition to achieving\\nthe same statistical guarantees as before, e-scores provide users flexibility\\nin adaptively choosing tolerance levels after observing the e-scores\\nthemselves, by upper bounding a post-hoc notion of error called size\\ndistortion. We experimentally demonstrate their efficacy in assessing LLM\\noutputs for different correctness types: mathematical factuality and property\\nconstraints satisfaction.\",\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2510.25770v1.pdf',\n",
       "  'published': '2025-10-29T17:59:16'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_arxiv_papers(query, max_results=5, days=7):\n",
    "    \"\"\"Fetch latest arXiv papers within the last N days.\"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    encoded_query = quote(query)\n",
    "    search_query = (\n",
    "        f\"search_query=all:{encoded_query}&start=0&max_results={max_results}\"\n",
    "        f\"&sortBy=submittedDate&sortOrder=descending\"\n",
    "    )\n",
    "    feed = feedparser.parse(base_url + search_query)\n",
    "\n",
    "    since_date = datetime.now() - timedelta(days=days)\n",
    "    papers = []\n",
    "    for entry in feed.entries:\n",
    "        published = datetime.strptime(entry.published, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "        if published >= since_date:\n",
    "            papers.append({\n",
    "                \"id\": entry.id,\n",
    "                \"title\": entry.title,\n",
    "                \"authors\": [a.name for a in entry.authors],\n",
    "                \"summary\": entry.summary,\n",
    "                \"pdf_url\": entry.id.replace(\"abs\", \"pdf\") + \".pdf\",\n",
    "                \"published\": published.isoformat()\n",
    "            })\n",
    "    return papers\n",
    "\n",
    "# Example usage\n",
    "papers = fetch_arxiv_papers(\"large language model\", max_results=3)\n",
    "papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b5eacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VFXMaster: Unlocking Dynamic Visual Effect Generation via\n",
      "In-Context Learning\n",
      "Baolu Li1∗, Yiming Zhang 1∗, Qinghe Wang 1,2∗† , Liqian Ma 3B, Xiaoyu Shi 2,\n",
      "Xintao Wang2, Pengfei Wan 2, Zhenfei Yin 4, Yunzhi Zhuge 1,\n",
      "Huchuan Lu1, Xu Jia 1B\n",
      "1Dalian University of Technology 2Kling Team, Kuaishou Technology 3ZMO AI Inc. 4Oxford University\n",
      "https://libaolu312.github.io/VFXMaster\n",
      "Target\n",
      "Image\n",
      "Ref.\n",
      "Video\n",
      "Target\n",
      "Image\n",
      "Ref.\n",
      "Video\n",
      "Target\n",
      "Image\n",
      "Ref.\n",
      "Video\n",
      "In-Domain Visual Effects\n",
      "Out-of-Domain Visual Effects\n"
     ]
    }
   ],
   "source": [
    "def download_pdf(paper, output_dir=\"downloads\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filename = f\"{output_dir}/{paper['id'].split('/')[-1]}.pdf\"\n",
    "    response = requests.get(paper[\"pdf_url\"])\n",
    "    if response.status_code == 200:\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        return filename\n",
    "    return None\n",
    "\n",
    "\n",
    "def read_pdf(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "# Example: Download and read one paper\n",
    "pdf_path = download_pdf(papers[0])\n",
    "raw_text = read_pdf(pdf_path)\n",
    "print(raw_text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22174bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 91\n",
      "VFXMaster: Unlocking Dynamic Visual Effect Generation via\n",
      "In-Context Learning\n",
      "Baolu Li1∗, Yiming Zhang 1∗, Qinghe Wang 1,2∗† , Liqian Ma 3B, Xiaoyu Shi 2,\n",
      "Xintao Wang2, Pengfei Wan 2, Zhenfei Yin 4, Yunzhi Zhuge 1,\n",
      "Huchuan Lu1, Xu Jia 1B\n",
      "1Dalian University of Technology 2Kling Team, Kuaishou Technology 3ZMO AI Inc. 4Oxford University\n",
      "https://libaolu312.github.io/VFXMaster\n",
      "Target\n",
      "Image\n",
      "Ref.\n",
      "Video\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=800, chunk_overlap=100):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "chunks = chunk_text(raw_text)\n",
    "print(f\"Total chunks: {len(chunks)}\")\n",
    "print(chunks[0][:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94909fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Stored embeddings in FAISS index.\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def store_in_faiss(chunks, paper_meta, save_path=\"faiss_index\"):\n",
    "    docs = [\n",
    "        Document(page_content=chunk, metadata=paper_meta)\n",
    "        for chunk in chunks\n",
    "    ]\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    vectorstore.save_local(save_path)\n",
    "    return vectorstore\n",
    "\n",
    "# Example\n",
    "vectorstore = store_in_faiss(chunks, papers[0])\n",
    "print(\" Stored embeddings in FAISS index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ce710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI=\"bolt://localhost:7687\"\n",
    "NEO4J_USER=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"testpassword\"\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER,NEO4J_PASSWORD))\n",
    "\n",
    "def store_paper_in_neo4j(paper,chunks):\n",
    "    with driver.session() as session:\n",
    "        session.run(\"\"\"\n",
    "            MERGE(p:Paper {id:$id})\n",
    "            SET p.title = $title , p.summary= $summary,\n",
    "                p.published =$published , p.authors = $authors        \n",
    "        \"\"\",parameters=paper)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            session.run(\"\"\"\n",
    "                MERGE (c:Chunk {paper_id: $paper_id, chunk_id: $chunk_id})\n",
    "                SET c.text = $text\n",
    "                MERGE (p:Paper {id: $paper_id})-[:HAS_CHUNK]->(c)\n",
    "            \"\"\", parameters={\n",
    "                \"paper_id\": paper[\"id\"],\n",
    "                \"chunk_id\": i,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "\n",
    "            store_paper_in_neo4j(papers[0], chunks)\n",
    "            print(\"✅ Paper + chunks stored in Neo4j graph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "276340c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Ingesting new arXiv papers for topic: Graph Neural Networks\n"
     ]
    },
    {
     "ename": "ConnectionAcquisitionTimeoutError",
     "evalue": "failed to obtain a connection from the pool within 60.0s (timeout)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionAcquisitionTimeoutError\u001b[39m         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Weekly ingestion completed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Run manually (in place of cron)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mweekly_ingest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGraph Neural Networks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mweekly_ingest\u001b[39m\u001b[34m(topic)\u001b[39m\n\u001b[32m      8\u001b[39m         chunks = chunk_text(text)\n\u001b[32m      9\u001b[39m         store_in_faiss(chunks, paper)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         \u001b[43mstore_paper_in_neo4j\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Weekly ingestion completed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mstore_paper_in_neo4j\u001b[39m\u001b[34m(paper, chunks)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[32m     16\u001b[39m     session.run(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m        MERGE (c:Chunk \u001b[39m\u001b[33m{\u001b[39m\u001b[33mpaper_id: $paper_id, chunk_id: $chunk_id})\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33m        SET c.text = $text\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: chunk\n\u001b[32m     24\u001b[39m     })\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mstore_paper_in_neo4j\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpapers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Paper + chunks stored in Neo4j graph.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mstore_paper_in_neo4j\u001b[39m\u001b[34m(paper, chunks)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[32m     16\u001b[39m     session.run(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m        MERGE (c:Chunk \u001b[39m\u001b[33m{\u001b[39m\u001b[33mpaper_id: $paper_id, chunk_id: $chunk_id})\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33m        SET c.text = $text\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: chunk\n\u001b[32m     24\u001b[39m     })\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mstore_paper_in_neo4j\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpapers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Paper + chunks stored in Neo4j graph.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "    \u001b[31m[... skipping similar frames: store_paper_in_neo4j at line 26 (97 times)]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mstore_paper_in_neo4j\u001b[39m\u001b[34m(paper, chunks)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[32m     16\u001b[39m     session.run(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m        MERGE (c:Chunk \u001b[39m\u001b[33m{\u001b[39m\u001b[33mpaper_id: $paper_id, chunk_id: $chunk_id})\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33m        SET c.text = $text\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: chunk\n\u001b[32m     24\u001b[39m     })\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[43mstore_paper_in_neo4j\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpapers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Paper + chunks stored in Neo4j graph.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mstore_paper_in_neo4j\u001b[39m\u001b[34m(paper, chunks)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstore_paper_in_neo4j\u001b[39m(paper,chunks):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m driver.session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m         \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[33;43m            MERGE(p:Paper \u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43mid:$id})\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[33;43m            SET p.title = $title , p.summary= $summary,\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[33;43m                p.published =$published , p.authors = $authors        \u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[33;43m        \u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[32m     16\u001b[39m             session.run(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m                MERGE (c:Chunk \u001b[39m\u001b[33m{\u001b[39m\u001b[33mpaper_id: $paper_id, chunk_id: $chunk_id})\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33m                SET c.text = $text\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: chunk\n\u001b[32m     24\u001b[39m             })\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\langchain2\\venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:305\u001b[39m, in \u001b[36mSession.run\u001b[39m\u001b[34m(self, query, parameters, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m     \u001b[38;5;28mself\u001b[39m._auto_result._buffer_all()\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdefault_access_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    307\u001b[39m cx = \u001b[38;5;28mself\u001b[39m._connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\langchain2\\venv\\Lib\\site-packages\\neo4j\\_sync\\work\\session.py:126\u001b[39m, in \u001b[36mSession._connect\u001b[39m\u001b[34m(self, access_mode, **acquire_kwargs)\u001b[39m\n\u001b[32m    124\u001b[39m     access_mode = \u001b[38;5;28mself\u001b[39m._config.default_access_mode\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccess_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43macquire_kwargs\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.CancelledError:\n\u001b[32m    130\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle_cancellation(message=\u001b[33m\"\u001b[39m\u001b[33m_connect\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\langchain2\\venv\\Lib\\site-packages\\neo4j\\_sync\\work\\workspace.py:181\u001b[39m, in \u001b[36mWorkspace._connect\u001b[39m\u001b[34m(self, access_mode, auth, **acquire_kwargs)\u001b[39m\n\u001b[32m    171\u001b[39m acquire_kwargs_ = {\n\u001b[32m    172\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maccess_mode\u001b[39m\u001b[33m\"\u001b[39m: access_mode,\n\u001b[32m    173\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: acquisition_deadline,\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdatabase_callback\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._make_db_resolution_callback(),\n\u001b[32m    179\u001b[39m }\n\u001b[32m    180\u001b[39m acquire_kwargs_.update(acquire_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28mself\u001b[39m._connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43macquire_kwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    183\u001b[39m     target_db.guessed\n\u001b[32m    184\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pinned_database\n\u001b[32m   (...)\u001b[39m\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# support SSR.\u001b[39;00m\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# => we need to fall back to explicit home database resolution\u001b[39;00m\n\u001b[32m    190\u001b[39m     log.debug(\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m[#0000]  _: <WORKSPACE> detected ssr support race; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mfalling back to explicit home database resolution\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    193\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\langchain2\\venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:678\u001b[39m, in \u001b[36mBoltPool.acquire\u001b[39m\u001b[34m(self, access_mode, timeout, database, bookmarks, auth, liveness_check_timeout, unprepared, database_callback)\u001b[39m\n\u001b[32m    671\u001b[39m deadline = acquisition_timeout_to_deadline(timeout)\n\u001b[32m    672\u001b[39m log.debug(\n\u001b[32m    673\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m[#0000]  _: <POOL> acquire direct connection, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    674\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maccess_mode=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m, database=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    675\u001b[39m     access_mode,\n\u001b[32m    676\u001b[39m     database,\n\u001b[32m    677\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m678\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mliveness_check_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munprepared\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\langchain2\\venv\\Lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:413\u001b[39m, in \u001b[36mIOPool._acquire\u001b[39m\u001b[34m(self, address, auth, deadline, liveness_check_timeout, unprepared)\u001b[39m\n\u001b[32m    408\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    409\u001b[39m             timeout == \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# deadline expired\u001b[39;00m\n\u001b[32m    410\u001b[39m             \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cond.wait(timeout)\n\u001b[32m    411\u001b[39m         ):\n\u001b[32m    412\u001b[39m             log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#0000]  _: <POOL> acquisition timed out\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ConnectionAcquisitionTimeoutError(\n\u001b[32m    414\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfailed to obtain a connection from the pool within \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    415\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeadline.original_timeout\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33ms (timeout)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    416\u001b[39m             )\n\u001b[32m    417\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33m[#0000]  _: <POOL> trying to hand out new connection\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection_creator()\n",
      "\u001b[31mConnectionAcquisitionTimeoutError\u001b[39m: failed to obtain a connection from the pool within 60.0s (timeout)"
     ]
    }
   ],
   "source": [
    "def weekly_ingest(topic=\"LLM\"):\n",
    "    print(f\"\\n Ingesting new arXiv papers for topic: {topic}\")\n",
    "    new_papers = fetch_arxiv_papers(topic, max_results=3)\n",
    "    for paper in new_papers:\n",
    "        pdf_path = download_pdf(paper)\n",
    "        if pdf_path:\n",
    "            text = read_pdf(pdf_path)\n",
    "            chunks = chunk_text(text)\n",
    "            store_in_faiss(chunks, paper)\n",
    "            store_paper_in_neo4j(paper, chunks)\n",
    "    print(\"✅ Weekly ingestion completed.\\n\")\n",
    "\n",
    "# Run manually (in place of cron)\n",
    "weekly_ingest(\"Graph Neural Networks\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
